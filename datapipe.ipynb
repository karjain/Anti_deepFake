{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datapipe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karjain/Anti_deepFake/blob/main/datapipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKaf32ElU_u1"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "from os.path import join\n",
        "import cv2\n",
        "import dlib\n",
        "import glob\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwJxq67LDczH",
        "outputId": "95ae7c69-5809-47e9-fde8-786a3e6a666c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "VCYbABwT6IuG",
        "outputId": "e4ef9938-b765-4e2c-eaba-5f83f42e8bf9"
      },
      "source": [
        "df = pd.read_json('/content/drive/MyDrive/Fake_dect/deepF/train_vid/metadata.json'\n",
        "                  ,orient='index').reset_index()\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "      <th>original</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aagfhgtpmv.mp4</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>train</td>\n",
              "      <td>vudstovrck.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aapnvogymq.mp4</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>train</td>\n",
              "      <td>jdubbvfswz.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abarnvbtwb.mp4</td>\n",
              "      <td>REAL</td>\n",
              "      <td>train</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>abofeumbvv.mp4</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>train</td>\n",
              "      <td>atvmxvwyns.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>abqwwspghj.mp4</td>\n",
              "      <td>FAKE</td>\n",
              "      <td>train</td>\n",
              "      <td>qzimuostzz.mp4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            index label  split        original\n",
              "0  aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\n",
              "1  aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\n",
              "2  abarnvbtwb.mp4  REAL  train            None\n",
              "3  abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\n",
              "4  abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TMaWmIddRIi"
      },
      "source": [
        "import glob\n",
        "\n",
        "video_files_path =  glob.glob('/content/drive/MyDrive/Fake_dect/deepF/train_vid/*.mp4')\n",
        "no_files=len(video_files_path)\n",
        "processed=0\n",
        "for video_file in video_files_path:\n",
        "  \n",
        "  reader = cv2.VideoCapture(video_file)\n",
        "  video_fn = video_file.split('/')[-1].split('.')[0]\n",
        "  video_name=video_fn+\".mp4\"\n",
        "  label = df.loc[df['index']==video_name,'label'].item()\n",
        "\n",
        "  #print(video_fn)\n",
        "\n",
        "  i=0\n",
        "  #os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "  fps = reader.get(cv2.CAP_PROP_FPS)\n",
        "  num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  #print(\"num of frames: \",num_frames)\n",
        " \n",
        "  # Face detector\n",
        "  face_detector = dlib.get_frontal_face_detector()\n",
        " \n",
        "  # Text variables\n",
        "  #font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
        " \n",
        "  # Frame numbers and length of output video\n",
        "  frame_num = 0\n",
        "  c=1\n",
        "  while reader.isOpened():\n",
        "      _, image = reader.read()\n",
        "      if image is None:\n",
        "          break\n",
        "      if c%33==0:\n",
        "        frame_num += 1\n",
        "  \n",
        "        # Image size\n",
        "        height, width = image.shape[:2]\n",
        "  \n",
        "  \n",
        "        # . Detect with dlib\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector(gray, 1)\n",
        "        if len(faces):\n",
        "            # For now only take biggest face\n",
        "            for face in faces:\n",
        "              # Face crop with dlib and bounding box scale enlargement\n",
        "              x, y, size = get_boundingbox(face, width, height)\n",
        "              cropped_face = image[y:y+size, x:x+size]\n",
        "              if( label=='FAKE' ):\n",
        "                cv2.imwrite(\"/content/drive/MyDrive/Fake_dect/deepF/train_img/fake/\"+video_fn+str(i)+'.jpg',cropped_face)\n",
        "              else:\n",
        "                cv2.imwrite(\"/content/drive/MyDrive/Fake_dect/deepF/train_img/real/\"+video_fn+str(i)+'.jpg',cropped_face)\n",
        "              \n",
        "              i+=1\n",
        "              #cv2.waitKey(33)\n",
        "        \n",
        "      c+=1\n",
        "  print(video_name, \" \", label)\n",
        "  print(\"total frames taken: \",frame_num)\n",
        "  processed+=1\n",
        "  print(\"Progress= \", (processed/no_files)*100  )\n",
        "      \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1BWNnctuktD"
      },
      "source": [
        "###Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNCN6zhzFuX"
      },
      "source": [
        "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
        "    \"\"\"\n",
        "    Expects a dlib face to generate a quadratic bounding box.\n",
        "    :param face: dlib face class\n",
        "    :param width: frame width\n",
        "    :param height: frame height\n",
        "    :param scale: bounding box size multiplier to get a bigger face region\n",
        "    :param minsize: set minimum bounding box size\n",
        "    :return: x, y, bounding_box_size in opencv form\n",
        "    \"\"\"\n",
        "    x1 = face.left()\n",
        "    y1 = face.top()\n",
        "    x2 = face.right()\n",
        "    y2 = face.bottom()\n",
        "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
        "    if minsize:\n",
        "        if size_bb < minsize:\n",
        "            size_bb = minsize\n",
        "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "\n",
        "    # Check for out of bounds, x-y top left corner\n",
        "    x1 = max(int(center_x - size_bb // 2), 0)\n",
        "    y1 = max(int(center_y - size_bb // 2), 0)\n",
        "    # Check for too big bb size for given x, y\n",
        "    size_bb = min(width - x1, size_bb)\n",
        "    size_bb = min(height - y1, size_bb)\n",
        "\n",
        "    return x1, y1, size_bb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKped_s3unJ4"
      },
      "source": [
        "def video_to_croped_frames(video_files_root, out_path, metafile_path, frame_rate=33):\n",
        "  file_extensions = ['mp4','avi']\n",
        "  #file_extension eg 'mp4', 'avi' \n",
        "  \n",
        "  df = pd.read_json(metafile_path, orient='index')                \n",
        "  video_files_path=[]\n",
        "  for ext in file_extensions:\n",
        "    video_files_path.extend(glob.glob(video_files_root + '/*.'+ext))\n",
        "    print(type(video_files_path))\n",
        "  no_files = len(video_files_path)\n",
        "\n",
        "  #os.makedirs(out_path+'/fake', exist_ok=True)\n",
        "  #os.makedirs(out_path+'/real', exist_ok=True)\n",
        "  \n",
        "  videos_processed=0\n",
        "\n",
        "  for video_file in video_files_path:\n",
        "    reader = cv2.VideoCapture(video_file)\n",
        "    \n",
        "    video_name = video_file.split('/')[-1]\n",
        "    video_fn = video_name.split('.')[0]\n",
        "\n",
        "    label = df.loc[video_name,'label']\n",
        "\n",
        "    #fps = reader.get(cv2.CAP_PROP_FPS)\n",
        "    #num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    #print(\"num of frames: \",num_frames)\n",
        "  \n",
        "    # Face detector\n",
        "    face_detector = dlib.get_frontal_face_detector()\n",
        "    \n",
        "    # Frame numbers and length of output video\n",
        "    i=0\n",
        "    c=1\n",
        "    while reader.isOpened():\n",
        "        _, image = reader.read()\n",
        "        if image is None:\n",
        "            break\n",
        "        if c%frame_rate==0:\n",
        "    \n",
        "          # Image size\n",
        "          height, width = image.shape[:2]\n",
        "\n",
        "          # . Detect with dlib\n",
        "          gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "          faces = face_detector(gray, 1)\n",
        "          if len(faces):\n",
        "              for face in faces:\n",
        "                # Face crop with dlib and bounding box scale enlargement\n",
        "                x, y, size = get_boundingbox(face, width, height)\n",
        "                cropped_face = image[y:y+size, x:x+size]\n",
        "                cropped_face = cv2.resize(cropped_face, (112, 112))\n",
        "                if( label=='FAKE' ):\n",
        "                  cv2.imwrite(out_path + \"/fake/\" + \"deepF\" + video_fn+str(i) + '.jpg', cropped_face)\n",
        "                else:\n",
        "                  cv2.imwrite(out_path + \"/real/\" + \"deepF\" + video_fn+str(i) + '.jpg', cropped_face)\n",
        "                \n",
        "                i+=1\n",
        "                #cv2.waitKey(33)\n",
        "          \n",
        "        c+=1\n",
        "    print(video_name, \" \", label)\n",
        "    print(\"total frames written: \",i)\n",
        "    videos_processed+=1\n",
        "    print(\"Progress= \", (videos_processed/no_files)*100  )\n",
        "      \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n533IGLh8XH",
        "outputId": "39857c03-1153-4a47-b803-013e044d9d64"
      },
      "source": [
        "video_to_croped_frames('/content/drive/MyDrive/Fake_dect/deepF/train_vid', '/content/drive/MyDrive/Fake_dect/deepF', '/content/drive/MyDrive/Fake_dect/deepF/train_vid/metadata.json', frame_rate=33)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "aagfhgtpmv.mp4   FAKE\n",
            "total frames written:  9\n",
            "Progress=  0.25\n",
            "aapnvogymq.mp4   FAKE\n",
            "total frames written:  14\n",
            "Progress=  0.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}