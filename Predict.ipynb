{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJGcsGDPwxGOueFQLJVGg2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karjain/Anti_deepFake/blob/main/Predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeQIc8QVNjkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a723b5c1-2fc1-46d8-8051-68ca1c169611"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "%cd /content/drive"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqIrA5E2OpaP"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import PIL.Image as Image\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_hub as hub\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "from os.path import join\n",
        "import cv2\n",
        "import dlib\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "from PIL import Image as pil_image\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWewLeCYPGia",
        "outputId": "b86c65ea-590b-42fc-81fe-9fd848ff8ccd"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkDLBBpqO5Wc"
      },
      "source": [
        "reload_model = tf.keras.models.load_model('/content/drive/MyDrive/Fake_dect/model.h5')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiOBU45oaItx"
      },
      "source": [
        "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
        "    \"\"\"\n",
        "    Expects a dlib face to generate a quadratic bounding box.\n",
        "    :param face: dlib face class\n",
        "    :param width: frame width\n",
        "    :param height: frame height\n",
        "    :param scale: bounding box size multiplier to get a bigger face region\n",
        "    :param minsize: set minimum bounding box size\n",
        "    :return: x, y, bounding_box_size in opencv form\n",
        "    \"\"\"\n",
        "    x1 = face.left()\n",
        "    y1 = face.top()\n",
        "    x2 = face.right()\n",
        "    y2 = face.bottom()\n",
        "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
        "    if minsize:\n",
        "        if size_bb < minsize:\n",
        "            size_bb = minsize\n",
        "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "\n",
        "    # Check for out of bounds, x-y top left corner\n",
        "    x1 = max(int(center_x - size_bb // 2), 0)\n",
        "    y1 = max(int(center_y - size_bb // 2), 0)\n",
        "    # Check for too big bb size for given x, y\n",
        "    size_bb = min(width - x1, size_bb)\n",
        "    size_bb = min(height - y1, size_bb)\n",
        "\n",
        "    return x1, y1, size_bb"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vimaeVCVVVUV"
      },
      "source": [
        "def video_to_croped_frames(video_files_path, output_path, frame_rate=33 ):\n",
        "\n",
        "\n",
        "  os.makedirs(output_path+'/test', exist_ok=True)\n",
        "    \n",
        "  videos_processed=0\n",
        "\n",
        "  reader = cv2.VideoCapture(video_files_path)\n",
        "  \n",
        "  video_name = video_files_path.split('/')[-1]\n",
        "  video_fn = video_name.split('.')[0]\n",
        "\n",
        "  # Face detector\n",
        "  face_detector = dlib.get_frontal_face_detector()\n",
        "  \n",
        "  # Frame numbers and length of output video\n",
        "  i=0\n",
        "  c=1\n",
        "  while reader.isOpened():\n",
        "      _, image = reader.read()\n",
        "      if image is None:\n",
        "          break\n",
        "      if c%frame_rate==1:\n",
        "  \n",
        "        # Image size\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # . Detect with dlib\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector(gray, 1)\n",
        "        if len(faces):\n",
        "            for face in faces:\n",
        "              # Face crop with dlib and bounding box scale enlargement\n",
        "              x, y, size = get_boundingbox(face, width, height)\n",
        "              cropped_face = image[y:y+size, x:x+size]\n",
        "              print(output_path + \"/test/\" + video_fn+str(i) + '.jpg')\n",
        "              cv2.imwrite(str(output_path + \"/test/\" + video_fn+str(i) + '.jpg'), cropped_face)\n",
        "              i+=1 \n",
        "      c+=1           \n",
        "        \n",
        "  print(video_name)\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiTj5wouYTQh",
        "outputId": "7719c9ae-ff4a-42fe-e62e-85ac88785bab"
      },
      "source": [
        "video_to_croped_frames('/content/drive/MyDrive/Fake_dect/adohdulfwb.mp4','/content/drive/MyDrive/Fake_dect/Trial_Dataset')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb0.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb1.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb2.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb3.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb4.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb5.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb6.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb7.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb8.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/adohdulfwb9.jpg\n",
            "adohdulfwb.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}