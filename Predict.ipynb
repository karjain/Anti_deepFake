{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSTWe74CSK/+7Cf202LiXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karjain/Anti_deepFake/blob/main/Predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeQIc8QVNjkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031d0a3a-da96-4c0b-d63f-5d9e80a6a4e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "%cd /content/drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqIrA5E2OpaP"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import PIL.Image as Image\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_hub as hub\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "from os.path import join\n",
        "import cv2\n",
        "import dlib\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "from PIL import Image as pil_image\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWewLeCYPGia",
        "outputId": "b86c65ea-590b-42fc-81fe-9fd848ff8ccd"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkDLBBpqO5Wc"
      },
      "source": [
        "reload_model = tf.keras.models.load_model('/content/drive/MyDrive/Fake_dect/modelK.h5')"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiOBU45oaItx"
      },
      "source": [
        "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
        "    \"\"\"\n",
        "    Expects a dlib face to generate a quadratic bounding box.\n",
        "    :param face: dlib face class\n",
        "    :param width: frame width\n",
        "    :param height: frame height\n",
        "    :param scale: bounding box size multiplier to get a bigger face region\n",
        "    :param minsize: set minimum bounding box size\n",
        "    :return: x, y, bounding_box_size in opencv form\n",
        "    \"\"\"\n",
        "    x1 = face.left()\n",
        "    y1 = face.top()\n",
        "    x2 = face.right()\n",
        "    y2 = face.bottom()\n",
        "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
        "    if minsize:\n",
        "        if size_bb < minsize:\n",
        "            size_bb = minsize\n",
        "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "\n",
        "    # Check for out of bounds, x-y top left corner\n",
        "    x1 = max(int(center_x - size_bb // 2), 0)\n",
        "    y1 = max(int(center_y - size_bb // 2), 0)\n",
        "    # Check for too big bb size for given x, y\n",
        "    size_bb = min(width - x1, size_bb)\n",
        "    size_bb = min(height - y1, size_bb)\n",
        "\n",
        "    return x1, y1, size_bb"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vimaeVCVVVUV"
      },
      "source": [
        "def video_to_croped_frames(video_files_path, output_path, frame_rate=33 ):\n",
        "\n",
        "\n",
        "  os.makedirs(output_path+'/test', exist_ok=True)\n",
        "    \n",
        "  videos_processed=0\n",
        "\n",
        "  reader = cv2.VideoCapture(video_files_path)\n",
        "  \n",
        "  video_name = video_files_path.split('/')[-1]\n",
        "  video_fn = video_name.split('.')[0]\n",
        "\n",
        "  # Face detector\n",
        "  face_detector = dlib.get_frontal_face_detector()\n",
        "  \n",
        "  # Frame numbers and length of output video\n",
        "  i=0\n",
        "  c=1\n",
        "  while reader.isOpened():\n",
        "      _, image = reader.read()\n",
        "      if image is None:\n",
        "          break\n",
        "      if c%frame_rate==1:\n",
        "  \n",
        "        # Image size\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # . Detect with dlib\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector(gray, 1)\n",
        "        if len(faces):\n",
        "            for face in faces:\n",
        "              # Face crop with dlib and bounding box scale enlargement\n",
        "              x, y, size = get_boundingbox(face, width, height)\n",
        "              cropped_face = image[y:y+size, x:x+size]\n",
        "              print(output_path + \"/test/\" + video_fn+str(i) + '.jpg')\n",
        "              cv2.imwrite(str(output_path + \"/test/\" + video_fn+str(i) + '.jpg'), cropped_face)\n",
        "              i+=1 \n",
        "      c+=1           \n",
        "        \n",
        "  print(video_name)\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqInzjJxfGKV"
      },
      "source": [
        "!rm /content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/*.jpg"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiTj5wouYTQh",
        "outputId": "aa6b4252-9779-4794-92db-368234328a95"
      },
      "source": [
        "#video_to_croped_frames('/content/drive/MyDrive/Fake_dect/adohdulfwb.mp4','/content/drive/MyDrive/Fake_dect/Trial_Dataset/test')\n",
        "#video_to_croped_frames('/content/drive/MyDrive/Fake_dect/abarnvbtwb.mp4','/content/drive/MyDrive/Fake_dect/Trial_Dataset/test')\n",
        "\n",
        "video_to_croped_frames('/content/drive/MyDrive/Fake_dect/aagfhgtpmv.mp4','/content/drive/MyDrive/Fake_dect/Trial_Dataset/test')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv0.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv1.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv2.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv3.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv4.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv5.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv6.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv7.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv8.jpg\n",
            "/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/test/aagfhgtpmv9.jpg\n",
            "aagfhgtpmv.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejIKbTA_qZpV"
      },
      "source": [
        "def create_ds(path, img_shape):\n",
        "  valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  str(path),\n",
        "  validation_split=0,\n",
        "  seed=123,\n",
        "  label_mode=None,\n",
        "  image_size=(img_shape[0], img_shape[1]),\n",
        "  batch_size=8)\n",
        "  \n",
        "  normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "  flip_layer = tf.keras.layers.experimental.preprocessing.RandomFlip(mode='horizontal')\n",
        "  contrast_layer = tf.keras.layers.experimental.preprocessing.RandomContrast(factor=(0,2))\n",
        "  \n",
        "  valid_ds = valid_ds.map(lambda x: (normalization_layer(x)))\n",
        "  valid_ds = valid_ds.map(lambda x: (flip_layer(x)))\n",
        "  valid_ds = valid_ds.map(lambda x: (contrast_layer(x)))\n",
        "  \n",
        "  #for image in valid_ds:\n",
        "  #  predictions = reload_model.predict(image)\n",
        "  #  print(predictions)\n",
        "  return valid_ds"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NALh-FGVqcRG",
        "outputId": "c55c7e2a-062e-492d-853e-9b148727779a"
      },
      "source": [
        "img_shape = [224,224]\n",
        "valid_ds = create_ds('/content/drive/MyDrive/Fake_dect/Trial_Dataset/test/',img_shape) \n",
        "#valid_ds = create_ds('/content/drive/MyDrive/Fake_dect/Trial_Dataset/real',img_shape) "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10 files belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Xu5XMpTFVG"
      },
      "source": [
        "for image in valid_ds:\n",
        "    predictions = reload_model.predict(image)\n",
        "    print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}